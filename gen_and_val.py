import torch
import torch.nn.functional as F
import numpy as np
import pickle
import os
import re
import chess
from model import GPT, GPTConfig  # Ensure model.py is in the same directory or adjust the import path accordingly
 

def remove_prefix_from_state_dict(state_dict, prefix='_orig_mod.'):
    """
    Removes a specified prefix from the state_dict keys.

    Args:
        state_dict (dict): Original state_dict with prefixed keys.
        prefix (str): Prefix to remove from each key.

    Returns:
        dict: Updated state_dict with prefixes removed.
    """
    flag = False
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith(prefix):
            new_key = key[len(prefix):]  # Remove the prefix
            flag = True
        else:
            new_key = key  # Keep the key as is
        new_state_dict[new_key] = value
    if flag:
        print('Unwanted prefixes were found in the checkpoint and have been removed.')
    return new_state_dict

def load_meta(data_dir):
    meta_path = os.path.join(data_dir, 'meta.pkl')
    if os.path.exists(meta_path):
        with open(meta_path, 'rb') as f:
            meta = pickle.load(f)
        vocab_size = meta['vocab_size']
        stoi = meta['stoi']  # String to index mapping
        itos = meta['itos']  # Index to string mapping
        return vocab_size, stoi, itos
    else:
        raise FileNotFoundError(f"Meta file not found at {meta_path}")

def tokenize(string, stoi):
    # Convert string to list of token IDs
    # Handle unknown characters by assigning a default index (e.g., <unk>)
    unk_index = stoi.get('<unk>', 0)
    return np.array([stoi.get(c, unk_index) for c in string], dtype=np.int64)

def detokenize(tokens, itos):
    # Convert list of token IDs back to string
    cs = ''.join([itos.get(t, '<unk>') for t in tokens])
    return cs

def load_model(checkpoint_path, device):
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_args = checkpoint['model_args']
    
    # Initialize model
    config = GPTConfig(**model_args)
    model = GPT(config)
    
    # Load state dict
    model.load_state_dict(remove_prefix_from_state_dict(checkpoint['model']))
    model.to(device)
    model.eval()  # Set model to evaluation mode
    return model

def generate_next_move(model, prompt_pgn, move_number, stoi, itos, device, verbose = False, max_length=1024, temperature=1.0):
    """
    Generates the next move in PGN format using the GPT model.

    Args:
        model (GPT): The GPT model.
        current_pgn (str): Current PGN string up to the last move.
        move_number (int): The current move number to generate (e.g., 3 for "3.").
        stoi (dict): String to index mapping.
        itos (dict): Index to string mapping.
        device (str): Device to run the model on ('cuda' or 'cpu').
        max_length (int): Maximum sequence length the model can handle.
        temperature (float): Sampling temperature for diversity.

    Returns:
        tuple: (generated_move, updated_pgn)
            - generated_move (str): The move generated by the model.
            - updated_pgn (str): The PGN string updated with the generated move.
    """
    # Determine if it's White's or Black's turn
    is_white_turn = (move_number % 2) == 1
    # Append the move prefix to the PGN to signal the model to generate a move

    generated_move = ""
    attempts = 0
    max_attempts = 10  # Prevent infinite loops

    while attempts < max_attempts:
        # Tokenize input

#        print('prompt pgn' , prompt_pgn, 'done')

        input_tokens = tokenize(prompt_pgn, stoi)
        input_tensor = torch.tensor(np.array([input_tokens]), dtype=torch.long).to(device)  # Shape: (1, seq_len)

        if input_tensor.size(1) > max_length:
            raise ValueError(f"Input sequence length {input_tensor.size(1)} exceeds maximum block size {max_length}.")

        with torch.no_grad():
            # Forward pass
            logits, _ = model(input_tensor)  # logits shape: (1, seq_len, vocab_size)
            # Focus on the last token's logits to predict the next token
            last_token_logits = logits[:, -1, :] / temperature  # Shape: (1, vocab_size)

            # Compute probabilities
            probabilities = F.softmax(last_token_logits, dim=-1)  # Shape: (1, vocab_size)

            # Sample a token
            sampled_token = torch.multinomial(probabilities, num_samples=1)  # Shape: (1, 1)

            # Convert token ID to character
            sampled_char = detokenize(sampled_token.cpu().tolist()[0], itos)

        # Check if space is generated, indicating the move is complete
        if sampled_char == ' ':
            break

        # Append the generated character to the move
        generated_move += sampled_char

        # Overwrite the move's position in the PGN with the generated character
        # Update the prompt_pgn to include the generated characters so far
##      this was the chatgpt logic, i prefer what I wrote below
##        prompt_pgn = f"{current_pgn}{move_prefix}{generated_move}"
        prompt_pgn = prompt_pgn + sampled_char
        attempts += 1


    return generated_move


def parse_pgn(pgn_string):
    """
    Parses the PGN string into individual moves.

    Args:
        pgn_string (str): The input PGN string.

    Returns:
        list: List of individual moves in order.
    """
    # Remove move numbers and result indicators
    pgn_clean = re.sub(r'\d+\.', '', pgn_string)
    pgn_clean = re.sub(r'1-0|0-1|1/2-1/2|\*', '', pgn_clean)
    # Split by spaces
    moves = pgn_clean.strip().split()
    return moves

def validate_move(board, move_san):
    """
    Validates if the move in SAN notation is legal.

    Args:
        board (chess.Board): Current game board.
        move_san (str): Move in Standard Algebraic Notation.

    Returns:
        bool: True if move is legal, False otherwise.
    """
    try:
        move = board.parse_san(move_san)
        return move in board.legal_moves
    except ValueError:
        return False

def run_validation(args):
    """
    Runs the move generation and validation process.

    Args:
        args: Parsed command-line arguments.
    """
    # Load meta information
    vocab_size, stoi, itos = load_meta(args.data_dir)
    print(f"Vocab Size: {vocab_size}")

    # Load the model
    model = load_model(args.checkpoint, args.device)
    print("Model loaded successfully.\n")

    # Parse the input PGN into individual moves
    input_pgn = args.input
    moves = parse_pgn(input_pgn[1:])
    print(f"Total Moves in Input PGN: {len(moves)}\n")

    # Initialize the chess board
    board = chess.Board()

    # Statistics
    total_generated_moves = 0
    illegal_moves_count = 0
    illegal_moves_examples = []
    illegal_move_positions = []
    current_pgn = ';'

    # Iterate through each move in the PGN
    for idx, move in enumerate(moves):
        move_number = idx + 1
#        print(f'Playing move {move_number}')
        player = "White" if board.turn == chess.WHITE else "Black"

        is_white_turn = (move_number % 2) == 1

        # Prepare the prompt for the model
        if is_white_turn:
            # For White's move, prepend the move number and a period
            move_prefix = f"{move_number//2 + 1}. "
        else:
            # For Black's move, just append a space
            move_prefix = ""
        prompt_pgn = current_pgn + move_prefix
        # Generate the next move
        generated_move = generate_next_move(model, prompt_pgn, move_number, stoi, itos, args.device, verbose = args.verbose, temperature=args.temperature)
        total_generated_moves += 1
        if args.verbose:
            print(f"Generated Move {move_number}: {generated_move}")

        # Validate the generated move
        is_legal = validate_move(board, generated_move)
        if is_legal:
            if args.verbose:
                print(f"Move {move_number}: Generated move '{generated_move}' is LEGAL.\n")
            illegal_move_positions.append(0)
        else:
            if args.verbose:
                print(f"Move {move_number}: Generated move '{generated_move}' is ILLEGAL.\n")
            illegal_moves_count += 1
            illegal_move_positions.append(1)
            illegal_moves_examples.append((move_number, player, generated_move, "Illegal move generated"))

        # Attempt to apply scripted move to the board
        try:
            board.push_san(move)
            if args.verbose:
                print(f"Move {move_number}: {player} plays {move}")
            current_pgn = prompt_pgn + move + ' '
        except ValueError:
            if args.verbose:
                print(f"Move {move_number}: {player} plays INVALID move {move}")
            continue

        next_move_number = move_number + 1

    # Reporting
    print("\n--- Validation Report ---")
    print(f"Total Generated Moves: {total_generated_moves}")
    print(f"Illegal Moves Generated: {illegal_moves_count}")
    if total_generated_moves > 0:
        frequency = (illegal_moves_count / total_generated_moves) * 100
        print(f"Frequency of Illegal Moves: {frequency:.2f}%\n")
    else:
        print("No moves were generated.\n")

    if illegal_moves_examples:
        print("Examples of Illegal Moves:")
        for example in illegal_moves_examples[:5]:  # Show up to 5 examples
            move_num, player, move, reason = example
            print(f"Move {move_num}: {player} -> '{move}' | Reason: {reason}")
    else:
        print("All generated moves were legal!")

    if args.graph:
        import matplotlib
        import matplotlib.pyplot as plt
        matplotlib.use('TkAgg')
        # Create scatter plot
        plt.plot(illegal_move_positions, marker = '.')

        # Add labels and title
        plt.xlabel('Moves')
        plt.ylabel('False')
        plt.title('False move spread')

        # Show plot
        plt.show()


def main():
    import argparse

    parser = argparse.ArgumentParser(description="GPT Model Inference and Move Validation for Chess PGN")
    parser.add_argument('--checkpoint', type=str, required=True, help='Path to the model checkpoint (e.g., checkpoint.pth)')
    parser.add_argument('--input', type=str, required=True, help='Input PGN string for which to predict and validate next moves')
    parser.add_argument('--data_dir', type=str, default='data/openwebtext', help='Directory where meta.pkl is located')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='Device to run the model on')
    parser.add_argument('--temperature', type=float, default=1.0, help='Sampling temperature for move generation (default: 1.0)')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output for debugging')
    parser.add_argument('--graph', action='store_true', help='Enable verbose output for debugging')

    args = parser.parse_args()

    run_validation(args)

if __name__ == "__main__":
    main()

#if __name__ == '__main__':
    #device = 'cpu'
    #vocab_size, stoi, itos = load_meta('data/lichess_hf_dataset')
    #model = load_model('checkpointBeta.pth', device)
    #generated_move, updated_pgn = generate_next_move(model, ';1. Nf3 d6 2. c4 Qd7', 5, stoi, itos, device)
    #print(generated_move)
    #print(updated_pgn)




