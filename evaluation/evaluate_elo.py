import argparse
import os
import pickle
import chess
import chess.engine
import torch
import torch.nn.functional as F
import numpy as np
from model import GPT, GPTConfig  # Ensure model.py is available
import json
import math
import sys

MODEL_DIR = "../models"
ELO_RESULTS_FILE = 'evaluation/elo_results.json'
torch.manual_seed(42)


# --- Utility functions ---
def load_json_file(filepath):
    """Load JSON file if it exists, otherwise return empty dict."""
    if os.path.exists(filepath):
        with open(filepath, 'r') as f:
            return json.load(f)
    return {}

def save_json_file(filepath, data):
    """Save data (dict) to JSON file."""
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=4)

 
def load_meta(data_dir):
    meta_path = os.path.join(data_dir, 'meta.pkl')
    if os.path.exists(meta_path):
        with open(meta_path, 'rb') as f:
            meta = pickle.load(f)
        return meta['vocab_size'], meta['stoi'], meta['itos']
    else:
        raise FileNotFoundError(f"Meta file not found at {meta_path}")


def load_model(checkpoint_path, device):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_args = checkpoint['model_args']
    config = GPTConfig(**model_args)
    model = GPT(config)
    state_dict = checkpoint['model']
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    return model

def tokenize(string, stoi):
    """
    Converts string to list of token IDs.

    Args:
        string (str): Input string.
        stoi (dict): String to index mapping.

    Returns:
        np.ndarray: Array of token IDs.
    """
    # Handle unknown characters by assigning a default index (e.g., <unk>)
    unk_index = stoi.get('<unk>', 0)
    return np.array([stoi.get(c, unk_index) for c in string], dtype=np.int64)

def detokenize(tokens, itos):
    """
    Converts list of token IDs back to string.

    Args:
        tokens (list): List of token IDs.
        itos (dict): Index to string mapping.

    Returns:
        str: Reconstructed string.
    """
    return ''.join([itos.get(t, '<unk>') for t in tokens])


def generate_next_move(model, prompt_pgn, stoi, itos, device, verbose=False, max_length=1023, temperature=1.0, troubleshoot_verbose = False):
    """
    Generates the next move in PGN format using the GPT model.

    Args:
        model (GPT): The GPT model.
        prompt_pgn (str): Current PGN string up to the last move.
        move_number (int): The current move number to generate (e.g., 3 for "3.").
        stoi (dict): String to index mapping.
        itos (dict): Index to string mapping.
        device (str): Device to run the model on ('cuda' or 'cpu').
        verbose (bool): Enable verbose output for debugging.
        max_length (int): Maximum sequence length the model can handle.
        temperature (float): Sampling temperature for diversity.

    Returns:
        str: The move generated by the model.
    """
    # Determine if it's White's or Black's turn
    generated_move = ""
    attempts = 0
    max_attempts = 9  # Prevent infinite loops
    while attempts < max_attempts:

        # Tokenize input
        input_tokens = tokenize(prompt_pgn, stoi)
        input_tensor = torch.from_numpy(input_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)

        if input_tensor.size(1) > max_length:
            return None
            raise ValueError(f"Input sequence length {input_tensor.size(1)} exceeds maximum block size {max_length}.")

        with torch.no_grad():
            # Forward pass
            try:
                logits, _ = model(input_tensor)  # logits shape: (1, seq_len, vocab_size)
            except Exception as e:
                if verbose:
                    print(f"Got the following error {e}")
                if troubleshoot_verbose:
                    print(f"we got to prompting with this prompt {prompt_pgn} of length {len(prompt_pgn)}")
                break

            # Focus on the last token's logits to predict the next token
            last_token_logits = logits[:, -1, :] / temperature  # Shape: (1, vocab_size)

            # Compute probabilities
            probabilities = F.softmax(last_token_logits, dim=-1)  # Shape: (1, vocab_size)

            # Sample a token
            sampled_token = torch.multinomial(probabilities, num_samples=1)  # Shape: (1, 1)

            # Convert token ID to character
            sampled_char = detokenize(sampled_token.cpu().tolist()[0], itos)

        # Check if space is generated, indicating the move is complete
        breaking_chars = [" ",";","#","+"]
        if sampled_char in breaking_chars: 
            break


        # Append the generated character to the move
        generated_move += sampled_char

        # Update the prompt_pgn to include the generated characters so far
        prompt_pgn += sampled_char
        attempts += 1

    return generated_move


def predict_next_characters(model, input_string, stoi, itos, device, max_length=1024):
    input_tokens = np.array([stoi[c] for c in input_string], dtype=np.int64)
    input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(device)

    if input_tensor.size(1) > max_length:
        raise ValueError(f"Input sequence length {input_tensor.size(1)} exceeds max block size {max_length}.")

    with torch.no_grad():
        logits, _ = model(input_tensor)
        probs = F.softmax(logits, dim=-1)
        predicted_tokens = torch.argmax(probs, dim=-1).squeeze(0).tolist()
        predicted_chars = ''.join([itos[t] for t in predicted_tokens])

    return predicted_chars

def chess_gpt_generated_move(model, board, prompt_pgn, stoi, itos, device, max_retries,idx,verbose,troubleshooting_verbose,beam_search,beam_width):
    retries = 0
    move = None
    invalid_generations = []
    if beam_search:
        sorted_moves = generate_next_move_beam_search(model, prompt_pgn, stoi, itos, device,beam_width=beam_width,verbose=verbose)
    while retries < max_retries:
        if beam_search:
            generated_move = sorted_moves[retries]
        else:
            generated_move = generate_next_move(model, prompt_pgn, stoi, itos, device,verbose=verbose)
        if generated_move == None:
            if verbose:
                print(f"When trying to generate move number {idx}, we exceded the size limits of the model") 
            return "oversize"
        else:
            try:
                board.push_san(generated_move)
                if verbose:
                    if generated_move.endswith('+'):

                        print(f'Generated check move, which is {board.is_check()}')
                return generated_move
            except ValueError:
               invalid_generations.append(generated_move)
               retries += 1
    if retries > 0:
        if troubleshooting_verbose:
            print(f'Prompted on input {prompt_pgn}')
        if verbose:
            if beam_search:
                print(f"On move {idx}, model generated the moves {sorted_moves[0:retries]}, which are/is invalid!")
            else:
                print(f"On move {idx}, model generated the moves {invalid_generations}, which are/is invalid!")
    return move

def generate_next_move_beam_search(model, prompt_pgn, stoi, itos, device, 
                                   beam_width=3, max_length=1023, temperature=1.0, verbose=False):
    """
    Uses beam search to generate a chess move (in PGN) from a given prompt.
    
    Args:
        model (GPT): The GPT model.
        prompt_pgn (str): The current PGN string (prompt).
        stoi (dict): Mapping from characters to token IDs.
        itos (dict): Mapping from token IDs to characters.
        device (str): Device to run the model on.
        beam_width (int): Number of beams to keep.
        max_length (int): Maximum number of tokens to generate.
        temperature (float): Temperature for scaling logits.
        verbose (bool): If True, print debugging information.
    
    Returns:
        str: The move generated (with trailing spaces stripped), or None if nothing was generated.
    """
    # Each beam is a tuple: (current_prompt, generated_move, cumulative_log_prob)
    beams = [(prompt_pgn, "", 0.0)]
    completed_beams = []
    end_chars = [" ",";","+","#"]
    for step in range(max_length):
        new_beams = []
        for current_prompt, generated, cum_log_prob in beams:
            # Check termination: if the last generated character is a space, we consider it finished.
            if generated and generated[-1] in end_chars:
                completed_beams.append((current_prompt, generated, cum_log_prob))
                continue

            # Tokenize the current prompt and run the model
            input_tokens = tokenize(current_prompt, stoi)
            input_tensor = torch.from_numpy(input_tokens).unsqueeze(0).to(device)  # shape: (1, seq_len)
            if input_tensor.size(1) > max_length:
                if verbose:
                    print(f"Input length {input_tensor.size(1)} exceeds max_length {max_length}.")
                continue

            with torch.no_grad():
                try:
                    logits, _ = model(input_tensor)
                except Exception as e:
                    if verbose:
                        print(f"Error during model inference: {e}")
                    continue

            # Focus on the logits for the last token and adjust by temperature
            last_logits = logits[:, -1, :] / temperature  # shape: (1, vocab_size)
            probabilities = F.softmax(last_logits, dim=-1).squeeze(0)  # shape: (vocab_size)
            
            # Get top beam_width tokens
            topk = torch.topk(probabilities, beam_width)
            top_probs = topk.values.cpu().numpy()
            top_indices = topk.indices.cpu().numpy()
            
            for prob, idx in zip(top_probs, top_indices):
                char = detokenize([idx], itos)
                new_generated = generated + char
                # Add log probability for numerical stability (log product becomes sum)
                new_cum_log_prob = cum_log_prob + math.log(prob + 1e-10)
                new_prompt = current_prompt + char
                new_beams.append((new_prompt, new_generated, new_cum_log_prob))
        
        if not new_beams:
            break

        # Keep only the beam_width most promising beams
        new_beams.sort(key=lambda x: x[2], reverse=True)
        beams = new_beams[:beam_width]

        # If all beams have finished (i.e. ended with a space), we can stop early.
        if all(beam[1].endswith((" ",";","+","#")) for beam in beams):
            completed_beams.extend(beams)
            break
    # If we have any completed beams, choose the one with the highest log probability.
    if completed_beams:
        completed_beams.sort(key=lambda x: x[2], reverse=True)
        best = completed_beams[0:beam_width]
        best_pgn = [move[1].strip() for move in best]
        best_move = completed_beams[0][1].strip()
    else:
        beams.sort(key=lambda x: x[2], reverse=True)
        best_move = beams[0][1].strip() if beams else ""
    
    if verbose:
        print(f"Beam search generated move: '{best_move}' with log-probability: {completed_beams[0][2] if completed_beams else beams[0][2]}")
    
    return best_pgn if best_pgn else None



def play_game_against_stockfish(model, engine, stoi, itos, device, stockfish_path, time_per_move, max_retries,color,verbose,troubleshooting_verbose,beam_search,beam_width):
    if verbose:
        print("------------------------- \n starting game against stockfish \n -------------------------")

    board = chess.Board()
    prompt_pgn = ';'
    move_number = 0
    color_slider = 0 if (color == "white") else 1
    if verbose:
        print(f"Our model is playing {color}")
    while not board.is_game_over():
        move_number += 1
        ##updating prompt pgn
        if move_number % 2 == 1:
            prompt_pgn += f"{move_number//2 + 1}."



        if (move_number + color_slider) % 2 == 1:
            ##gpt turn
            gpt_move = chess_gpt_generated_move(model, board, prompt_pgn, stoi, itos, device, max_retries, move_number,verbose,troubleshooting_verbose,beam_search,beam_width) 
            if gpt_move == None:
                if verbose:
                    print(f"Game lost for inability to generate valid move (max retries: {max_retries})")
                return "loss_invalid_gen",move_number
            elif gpt_move == "oversize":
                if verbose:
                    print(f"Game lost because of context size")
                return "loss_context_size",move_number
            else:
                pgn_move = gpt_move
        else:
            ##stockfish turn
            result = engine.play(board, chess.engine.Limit(time=time_per_move))
            pgn_stockfish_move = board.san(result.move)
            board.push(result.move)
            pgn_move = pgn_stockfish_move

        prompt_pgn += pgn_move + ' '
    if verbose:
        print(f"final pgn: {prompt_pgn}")
    if (move_number % 2 == 0 and color == "black") or (move_number % 2 == 1 and color == "white"):
        return 'win', move_number
    else:
        return 'loss',move_number


def update_elo(elo_a, elo_b, result, k=32):
    if result == "loss_context_size":
        return elo_a
    ##we count context size as not a valid game
    prob_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))
    score_a = 1 if result == "win" else 0 if ((result == "loss") or (result == "loss_invalid_gen")) else 0.5
    return elo_a + k * (score_a - prob_a)

def print_evaluation_report(args,entry_key):
    if args.beam_search:
        print("This elo was evaluated using beam search")
    elo_results = args.elo_results[entry_key]
    model_name = elo_results["model_name"]
    stockfish_name = elo_results["stockfish_name"]
    counters = elo_results["counters"]
    ng = counters["games_evaluated"]
    print("--------------------EVALUATION REPORT--------------------")
    print(f"Model {model_name} vs {stockfish_name}")
    print(f"Total games: {ng}")
    print(f"Wins: {counters["win"]} ({100 * (counters["win"]/ng):.2f}%)")
    print(f"Loss: {counters["loss"]} ({100 * (counters["loss"]/ng):.2f}%)")
    print(f"Context size: {counters["context_size"]} ({100 * (counters["context_size"]/ng):.2f}%)")
    print(f"Invalid generation: {counters["invalid_gen"]} ({100 * (counters["invalid_gen"]/ng):.2f}%)")

def quick_save_checkpoint(entry_key,game_idx, results_list, elo, QUICK_SAVE_FILE, quick_save):
    quick_save[entry_key] = {
        "games_so_far": results_list,
        "current_elo": elo,
        "num_games_evaluated": game_idx + 1
    }
    save_json_file(QUICK_SAVE_FILE, quick_save)
    print(f"Quick-saved checkpoint at game {game_idx+1}.")

def get_checkpoint_path(checkpoint):
    model_name = "_".join(checkpoint.split("_")[:-1])
    model_name_iteration_pth = checkpoint + ".pth"
    checkpoint_path = os.path.join(MODEL_DIR,model_name,model_name_iteration_pth)
    return checkpoint_path

def update_memory_stats(entry_key,elo_results,results_list,elo,counters,game_idx):
        counters["games_evaluated"] = game_idx + 1
        elo_results[entry_key]["games"] = results_list
        elo_results[entry_key]["final_elo"] = elo
        elo_results[entry_key]["counters"] = counters



def run_evaluation(args):
    
    """
    Evaluates the ELO of a given model by playing up to 100 games against Stockfish.
    - Resumes if the model+stockfish combo has been partially evaluated.
    - Skips if already completed 100 games.
    - Saves progress to 'quick_save.json' every 20 games (wipes it after a successful checkpoint or upon completion).
    - Stores final results in 'elo_results.json'.
    """
    
    # Where we store final results
    # Where we store partial results (quick save)
    QUICK_SAVE_FILE = 'evaluation/elo_quick_save.json'
    counters = {"evaluated_games": 0,"invalid_gen": 0,"context_size": 0,"loss": 0,"win": 0,}
    # Load model metadata
    vocab_size, stoi, itos = load_meta(args.data_dir)
    
    model = load_model(get_checkpoint_path(args.checkpoint), args.device)

    # Basic info about this run
    stockfish_path = args.stockfish_path
    
    model_name, stockfish_name = os.path.basename(args.checkpoint) , os.path.basename(stockfish_path)

    # We define a unique key to identify the combination of (model, stockfish)
    if args.beam_search:
        entry_key = f"beam_{model_name}_{stockfish_name}_{args.desired_elo}"
    else:
        entry_key = f"{model_name}_{stockfish_name}_{args.desired_elo}"
    print(f"entry key is {entry_key}")
    # If this entry already exists, check how many games are done
    if entry_key in elo_results:
        existing_entry = elo_results[entry_key]
        completed_games = existing_entry["counters"].get("games_evaluated", 0) 
        
        if completed_games >= args.evaluation_games:
            # This means the model+stockfish combo is already fully evaluated
            print(f"Skipping evaluation: {model_name} vs {stockfish_name} already has {args.evaluation_games} games.")
            return entry_key
        else:
            # Resume from partial results
            print(
                f"Resuming evaluation for {model_name} vs {stockfish_name} "
                f"at game {completed_games+1} with ELO {existing_entry['final_elo']}."
            )
            elo = existing_entry["final_elo"]
            start_game_idx = completed_games
            results_list = existing_entry["games"]
            counters = existing_entry["counters"]
    else:
        # Start from scratch
        print(f"Starting new evaluation for {model_name} vs {stockfish_name}.")
        elo = 1200  # Starting Elo
        start_game_idx = 0
        results_list = []

        # Create initial dictionary structure for this run
        elo_results[entry_key] = {
            "model_name": model_name,
            "stockfish_name": stockfish_name,
            "games": results_list,
            "final_elo": elo,
            "counters": counters
        }

    # Also check if there's a quick_save from a previous interruption
    quick_save = load_json_file(QUICK_SAVE_FILE)
    if entry_key in quick_save:
        quick_data = quick_save[entry_key]
        if quick_data["num_games_evaluated"] > start_game_idx:
            # Means we have a more up-to-date checkpoint in quick_save
            print(f"Found quick_save checkpoint. Resuming from quick_save data.")
            start_game_idx = quick_data["num_games_evaluated"]
            elo = quick_data["current_elo"]
            results_list = quick_data["games_so_far"]

    # The total number of games we want to evaluate
    total_games = args.evaluation_games

    # Start Stockfish engine
    engine = chess.engine.SimpleEngine.popen_uci(stockfish_path)
        # Set desired Elo rating
    max_elo = 3500      # Adjust based on your Stockfish version
    desired_elo = args.desired_elo
    # Ensure the desired Elo is within the valid range
    if desired_elo < 0 or desired_elo > max_elo:
        raise ValueError(f"Elo rating must be between 0 and {max_elo}")

    # Configure UCI options
    engine.configure({
        "UCI_LimitStrength": True,  # Enable strength limitation
        "UCI_Elo": args.desired_elo      # Set the Elo rating
    })

    # Check if the configuration was applied successfully
    print(f"Stockfish is now configured with Elo {args.desired_elo}.")


    # Now proceed with the evaluation from `start_game_idx+1` to `total_games`.
    for game_idx in range(start_game_idx, total_games):
        # Here you decide color. For simplicity, let's alternate color:
        color = "white" if game_idx % 2 == 0 else "black"

        # Play the game
        # result = play_game_against_stockfish(...) 
        # NOTE: In your snippet, you also had:
        #   result = play_game_against_stockfish(
        #       model, stoi, itos, args.device, stockfish_path, 
        #       args.time_per_move, args.max_retries, args.verbose
        #   )
        # We'll assume it returns: (result, num_moves)
        # For demonstration, let's mock them:
        ##TODO input color to the game and have it play the correct color
        result, num_moves = play_game_against_stockfish(
            model, engine, stoi, itos, args.device, 
            stockfish_path, args.time_per_move, 
            args.max_retries, color, args.verbose, args.troubleshooting_verbose,
            args.beam_search,args.beam_width
        )
        if result == "loss_invalid_gen":
            counters["invalid_gen"] += 1
        elif result == "loss_context_size":
            counters["context_size"] += 1
        elif result == "loss":
            counters["loss"] += 1
        else:
            counters["win"] += 1



        # Update ELO
        #   stockfish_elo is presumably some baseline or same as ours, or might be separate
        #   In your snippet, you used "elo = update_elo(elo, stockfish_elo, result)"
        ##TODO check this
        elo = update_elo(elo, args.desired_elo, result)
        if args.verbose:
            print(f"Game {game_idx} finished after {num_moves} moves, result: {result}, updated elo is {elo}")

        # Store this result
        game_info = {
            "color": color,
            "num_moves": num_moves,
            "elo": elo,
            "result": result,   # e.g., 1=win, 0.5=draw, 0=loss, or any scheme
            "game_index": game_idx + 1
        }
        results_list.append(game_info)

        # Update in-memory stats
        # Quick save every 20 games (and if itâ€™s not the final game)
        if (game_idx + 1) % 20 == 0 and (game_idx + 1) < total_games:
            quick_save_checkpoint(entry_key,game_idx, results_list, elo, QUICK_SAVE_FILE, quick_save)

    update_memory_stats(entry_key,elo_results,results_list,elo,counters,game_idx)
    # - Save final results in the main ELO results file
    save_json_file(ELO_RESULTS_FILE, elo_results)

    # - Clean up the quick_save for this entry (to allow fresh restarts next time)
    if entry_key in quick_save:
        del quick_save[entry_key]
        save_json_file(QUICK_SAVE_FILE, quick_save)

    engine.quit()
    return entry_key


def evaluate_models(model_names, args):
    for model_name in model_names:
        args.checkpoint = model_name
        args.save_file = os.path.join(args.save_dir, f"{model_name}_results.pkl")
        entry_key = run_evaluation(args)
        print_evaluation_report(args,entry_key)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate LLM against Stockfish to compute Elo.")
    parser.add_argument('--checkpoint', type=str, help='Path to model checkpoint.')
    parser.add_argument('--data_dir', type=str, default='data', help='Directory with meta.pkl.')
    parser.add_argument('--device', type=str, default='cuda', help='Device to run the model on.')
    parser.add_argument('--num_games', type=int, default=100, help='Number of games to play.')
    parser.add_argument('--time_per_move', type=float, default=0.1, help='Time per move for Stockfish (seconds).')
    parser.add_argument('--max_retries', type=int, default=3, help='Max retries for invalid LLM moves.')
    parser.add_argument('--evaluation_games', type=int, default=3, help='Max retries for invalid LLM moves.')
    parser.add_argument('--desired_elo', type=int, default=3, help='Max retries for invalid LLM moves.')
    parser.add_argument('--save_interval', type=int, default=10, help='Save progress every N games.')
    parser.add_argument('--save_file', type=str, default='results.pkl', help='File to save progress.')
    parser.add_argument('--models_dir', type=str, help='Directory with model checkpoints.')
    parser.add_argument('--save_dir', type=str, help='Directory to save results.')
    parser.add_argument('--stockfish_path', type=str, required=True, help='Path to Stockfish executable.')
    parser.add_argument('--verbose', action = "store_true", help='Path to Stockfish executable.')
    parser.add_argument('--troubleshooting_verbose', action = "store_true", help='Path to Stockfish executable.')

    parser.add_argument('--beam_width', type=int, default=3, help='Number of beams to use in beam search.')
    parser.add_argument('--beam_search', action= "store_true", help='Number of beams to use in beam search.')
    args = parser.parse_args()

    # Load or create main results dictionary
    elo_results = load_json_file(ELO_RESULTS_FILE)
    args.elo_results = elo_results

    if args.models_dir:
        model_names = [f[:-4] for f in os.listdir(args.models_dir) if f.endswith('.pth')]
        evaluate_models(model_names, args)
    else:
        entry_key = run_evaluation(args)
        print_evaluation_report(args,entry_key)
