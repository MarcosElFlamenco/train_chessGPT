import argparse
import os
import pickle
import chess
import chess.engine
import torch
import torch.nn.functional as F
import numpy as np
from model import GPT, GPTConfig  # Ensure model.py is available
import json



def load_meta(data_dir):
    meta_path = os.path.join(data_dir, 'meta.pkl')
    if os.path.exists(meta_path):
        with open(meta_path, 'rb') as f:
            meta = pickle.load(f)
        return meta['vocab_size'], meta['stoi'], meta['itos']
    else:
        raise FileNotFoundError(f"Meta file not found at {meta_path}")


def load_model(checkpoint_path, device):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_args = checkpoint['model_args']
    config = GPTConfig(**model_args)
    model = GPT(config)
    state_dict = checkpoint['model']
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    return model

def tokenize(string, stoi):
    """
    Converts string to list of token IDs.

    Args:
        string (str): Input string.
        stoi (dict): String to index mapping.

    Returns:
        np.ndarray: Array of token IDs.
    """
    # Handle unknown characters by assigning a default index (e.g., <unk>)
    unk_index = stoi.get('<unk>', 0)
    return np.array([stoi.get(c, unk_index) for c in string], dtype=np.int64)

def detokenize(tokens, itos):
    """
    Converts list of token IDs back to string.

    Args:
        tokens (list): List of token IDs.
        itos (dict): Index to string mapping.

    Returns:
        str: Reconstructed string.
    """
    return ''.join([itos.get(t, '<unk>') for t in tokens])


def generate_next_move(model, prompt_pgn, stoi, itos, device, verbose=False, max_length=1023, temperature=1.0, troubleshoot_verbose = False):
    """
    Generates the next move in PGN format using the GPT model.

    Args:
        model (GPT): The GPT model.
        prompt_pgn (str): Current PGN string up to the last move.
        move_number (int): The current move number to generate (e.g., 3 for "3.").
        stoi (dict): String to index mapping.
        itos (dict): Index to string mapping.
        device (str): Device to run the model on ('cuda' or 'cpu').
        verbose (bool): Enable verbose output for debugging.
        max_length (int): Maximum sequence length the model can handle.
        temperature (float): Sampling temperature for diversity.

    Returns:
        str: The move generated by the model.
    """
    # Determine if it's White's or Black's turn
    generated_move = ""
    attempts = 0
    max_attempts = 9  # Prevent infinite loops
    while attempts < max_attempts:

        # Tokenize input
        input_tokens = tokenize(prompt_pgn, stoi)
        input_tensor = torch.from_numpy(input_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)

        if input_tensor.size(1) > max_length:
            return None
            raise ValueError(f"Input sequence length {input_tensor.size(1)} exceeds maximum block size {max_length}.")

        with torch.no_grad():
            # Forward pass
            try:
                logits, _ = model(input_tensor)  # logits shape: (1, seq_len, vocab_size)
            except Exception as e:
                if verbose:
                    print(f"Got the following error {e}")
                    print(f"we got to prompting with this prompt {prompt_pgn} of length {len(prompt_pgn)}")
                break

            # Focus on the last token's logits to predict the next token
            last_token_logits = logits[:, -1, :] / temperature  # Shape: (1, vocab_size)

            # Compute probabilities
            probabilities = F.softmax(last_token_logits, dim=-1)  # Shape: (1, vocab_size)

            # Sample a token
            sampled_token = torch.multinomial(probabilities, num_samples=1)  # Shape: (1, 1)

            # Convert token ID to character
            sampled_char = detokenize(sampled_token.cpu().tolist()[0], itos)

        # Check if space is generated, indicating the move is complete
        if sampled_char == ' ':
            break

        # Append the generated character to the move
        generated_move += sampled_char

        # Update the prompt_pgn to include the generated characters so far
        prompt_pgn += sampled_char
        attempts += 1

    return generated_move


def predict_next_characters(model, input_string, stoi, itos, device, max_length=1024):
    input_tokens = np.array([stoi[c] for c in input_string], dtype=np.int64)
    input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(device)

    if input_tensor.size(1) > max_length:
        raise ValueError(f"Input sequence length {input_tensor.size(1)} exceeds max block size {max_length}.")

    with torch.no_grad():
        logits, _ = model(input_tensor)
        probs = F.softmax(logits, dim=-1)
        predicted_tokens = torch.argmax(probs, dim=-1).squeeze(0).tolist()
        predicted_chars = ''.join([itos[t] for t in predicted_tokens])

    return predicted_chars

def chess_gpt_generated_move(model, board, prompt_pgn, stoi, itos, device, max_retries,idx,verbose):
    retries = 0
    move = None
    while retries < max_retries:
        generated_move = generate_next_move(model, prompt_pgn, stoi, itos, device,verbose=verbose)
        if generated_move == None:
            print(f"When trying to generate move number {idx}, we exceded the size limtis of the model") 
            retries +=1
        else:
            try:
                board.push_san(generated_move)
                if verbose:
                    print(f'Valid move generated, now pushing {generated_move}')
                return generated_move
                break
            except ValueError:
                if verbose:
                    print(f"On move {idx}, model generated the move {generated_move}, which is invalid!")
                retries += 1
    return move


def play_game_against_stockfish(model, stoi, itos, device, stockfish_path, time_per_move, max_retries,color,verbose):
    if verbose:
        print("------------------------- \n starting game against stockfish \n -------------------------")
    engine = chess.engine.SimpleEngine.popen_uci(stockfish_path)
    board = chess.Board()
    prompt_pgn = ';'
    move_number = 0
    color_slider = 0 if (color == "white") else 1
    if verbose:
        print(f"Our model is playing {color}")
    while not board.is_game_over():
        move_number += 1
        if verbose:
            print(f'move number {move_number}')
        ##updating prompt pgn
        if move_number % 2 == 1:
            prompt_pgn += f"{move_number//2 + 1}."



        if (move_number + color_slider) % 2 == 1:
            ##gpt turn
            gpt_move = chess_gpt_generated_move(model, board, prompt_pgn, stoi, itos, device, max_retries, move_number,verbose) 
            if gpt_move == None:
                engine.quit()
                print(f"Game lost for inability to generate valid move (max retries: {max_retries})")
                return "loss_by_no_gen",move_number
            else:
                pgn_move = gpt_move
        else:
            ##stockfish turn
            result = engine.play(board, chess.engine.Limit(time=time_per_move))
            pgn_stockfish_move = board.san(result.move)
            if verbose:
                print(f"Stockfish plays {pgn_stockfish_move}")
            board.push(result.move)
            pgn_move = pgn_stockfish_move

        prompt_pgn += pgn_move + ' '
    engine.quit()

    if (move_number % 2 == 0 and color == "black") or (move_number % 2 == 1 and color == "white"):
        return 'win', move_number
    else:
        return 'loss',move_number


def update_elo(elo_a, elo_b, result, k=32):
    prob_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))
    score_a = 1 if result == "win" else 0 if result == "loss" else 0.5
    return elo_a + k * (score_a - prob_a)

def run_evaluation(args):
    """
    Evaluates the ELO of a given model by playing up to 100 games against Stockfish.
    - Resumes if the model+stockfish combo has been partially evaluated.
    - Skips if already completed 100 games.
    - Saves progress to 'quick_save.json' every 20 games (wipes it after a successful checkpoint or upon completion).
    - Stores final results in 'elo_results.json'.
    """
    
    # --- Utility functions ---
    def load_json_file(filepath):
        """Load JSON file if it exists, otherwise return empty dict."""
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                return json.load(f)
        return {}

    def save_json_file(filepath, data):
        """Save data (dict) to JSON file."""
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=4)

    # --- Prepare evaluation and IO ---
    # Where we store final results
    ELO_RESULTS_FILE = 'evaluation/elo_results.json'
    # Where we store partial results (quick save)
    QUICK_SAVE_FILE = 'evaluation/elo_quick_save.json'
    no_gen_counter = 0
    # Load model metadata
    vocab_size, stoi, itos = load_meta(args.data_dir)
    
    # Prepare absolute or relative path for your checkpoint
    args.checkpoint = 'evaluation/eval_models/' + args.checkpoint + '.pth'
    model = load_model(args.checkpoint, args.device)

    # Basic info about this run
    stockfish_path = args.stockfish_path
    model_name = os.path.basename(args.checkpoint)  # e.g., "mymodel.pth"
    stockfish_name = os.path.basename(stockfish_path)  # e.g., "stockfish_15_x64"

    # Load or create main results dictionary
    elo_results = load_json_file(ELO_RESULTS_FILE)

    # We define a unique key to identify the combination of (model, stockfish)
    entry_key = f"{model_name}_{stockfish_name}"

    # If this entry already exists, check how many games are done
    if entry_key in elo_results:
        existing_entry = elo_results[entry_key]
        completed_games = existing_entry.get("num_games_evaluated", 0)
        
        if completed_games >= args.evaluation_games:
            # This means the model+stockfish combo is already fully evaluated
            print(f"Skipping evaluation: {model_name} vs {stockfish_name} already has {args.evaluation_games} games.")
            return existing_entry["final_elo"]
        else:
            # Resume from partial results
            print(
                f"Resuming evaluation for {model_name} vs {stockfish_name} "
                f"at game {completed_games+1} with ELO {existing_entry['final_elo']}."
            )
            elo = existing_entry["final_elo"]
            start_game_idx = completed_games
            results_list = existing_entry["games"]
            no_gen_counter = existing_entry['no_gen_counter']
    else:
        # Start from scratch
        print(f"Starting new evaluation for {model_name} vs {stockfish_name}.")
        elo = 1200  # Starting Elo
        start_game_idx = 0
        results_list = []

        # Create initial dictionary structure for this run
        elo_results[entry_key] = {
            "model": model_name,
            "stockfish": stockfish_name,
            "games": results_list,
            "final_elo": elo,
            "num_games_evaluated": 0
        }

    # Also check if there's a quick_save from a previous interruption
    quick_save = load_json_file(QUICK_SAVE_FILE)
    if entry_key in quick_save:
        quick_data = quick_save[entry_key]
        if quick_data["num_games_evaluated"] > start_game_idx:
            # Means we have a more up-to-date checkpoint in quick_save
            print(f"Found quick_save checkpoint. Resuming from quick_save data.")
            start_game_idx = quick_data["num_games_evaluated"]
            elo = quick_data["current_elo"]
            results_list = quick_data["games_so_far"]

    # The total number of games we want to evaluate
    total_games = args.evaluation_games

    # Now proceed with the evaluation from `start_game_idx+1` to `total_games`.
    for game_idx in range(start_game_idx, total_games):
        # Here you decide color. For simplicity, let's alternate color:
        color = "white" if game_idx % 2 == 0 else "black"

        # Play the game
        # result = play_game_against_stockfish(...) 
        # NOTE: In your snippet, you also had:
        #   result = play_game_against_stockfish(
        #       model, stoi, itos, args.device, stockfish_path, 
        #       args.time_per_move, args.max_retries, args.verbose
        #   )
        # We'll assume it returns: (result, num_moves)
        # For demonstration, let's mock them:
        ##TODO input color to the game and have it play the correct color
        result, num_moves = play_game_against_stockfish(
            model, stoi, itos, args.device, 
            stockfish_path, args.time_per_move, 
            args.max_retries, color, args.verbose
        )
        if result == "loss_by_no_gen":
            no_gen_counte += 1

        # Update ELO
        #   stockfish_elo is presumably some baseline or same as ours, or might be separate
        #   In your snippet, you used "elo = update_elo(elo, stockfish_elo, result)"
        ##TODO check this
        stockfish_elo = 1200
        elo = update_elo(elo, stockfish_elo, result)
        print(f"Game {game_idx} finished after {num_moves} moves, result: {result}, updated elo is {elo}")

        # Store this result
        game_info = {
            "color": color,
            "num_moves": num_moves,
            "elo": elo,
            "result": result,   # e.g., 1=win, 0.5=draw, 0=loss, or any scheme
            "game_index": game_idx + 1
        }
        results_list.append(game_info)

        # Update in-memory stats
        elo_results[entry_key]["games"] = results_list
        elo_results[entry_key]["final_elo"] = elo
        elo_results[entry_key]["num_games_evaluated"] = game_idx + 1
        elo_results[entry_key]["no_gen_counter"] = no_gen_counter

        # Quick save every 20 games (and if itâ€™s not the final game)
        if (game_idx + 1) % 20 == 0 and (game_idx + 1) < total_games:
            quick_save[entry_key] = {
                "games_so_far": results_list,
                "current_elo": elo,
                "num_games_evaluated": game_idx + 1
            }
            save_json_file(QUICK_SAVE_FILE, quick_save)
            print(f"Quick-saved checkpoint at game {game_idx+1}.")

    # Once we've reached 100 games or finished the loop:
    # - Save final results in the main ELO results file
    save_json_file(ELO_RESULTS_FILE, elo_results)

    # - Clean up the quick_save for this entry (to allow fresh restarts next time)
    if entry_key in quick_save:
        del quick_save[entry_key]
        save_json_file(QUICK_SAVE_FILE, quick_save)

    print(f"Evaluation complete for {model_name} vs {stockfish_name}. Final ELO: {elo}.")
    return elo


def evaluate_models(model_names, args):
    for model_name in model_names:
        args.checkpoint = os.path.join(args.models_dir, f"{model_name}.pth")
        args.save_file = os.path.join(args.save_dir, f"{model_name}_results.pkl")
        final_elo = run_evaluation(args)
        print(f"Model: {model_name}, Final Elo: {final_elo}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate LLM against Stockfish to compute Elo.")
    parser.add_argument('--checkpoint', type=str, help='Path to model checkpoint.')
    parser.add_argument('--data_dir', type=str, default='data', help='Directory with meta.pkl.')
    parser.add_argument('--device', type=str, default='cuda', help='Device to run the model on.')
    parser.add_argument('--num_games', type=int, default=100, help='Number of games to play.')
    parser.add_argument('--time_per_move', type=float, default=0.1, help='Time per move for Stockfish (seconds).')
    parser.add_argument('--max_retries', type=int, default=3, help='Max retries for invalid LLM moves.')
    parser.add_argument('--evaluation_games', type=int, default=3, help='Max retries for invalid LLM moves.')
    parser.add_argument('--save_interval', type=int, default=10, help='Save progress every N games.')
    parser.add_argument('--save_file', type=str, default='results.pkl', help='File to save progress.')
    parser.add_argument('--models_dir', type=str, help='Directory with model checkpoints.')
    parser.add_argument('--save_dir', type=str, help='Directory to save results.')
    parser.add_argument('--stockfish_path', type=str, required=True, help='Path to Stockfish executable.')
    parser.add_argument('--verbose', action = "store_true", help='Path to Stockfish executable.')
    args = parser.parse_args()


    if args.models_dir:
        model_names = [f[:-4] for f in os.listdir(args.models_dir) if f.endswith('.pth')]
        evaluate_models(model_names, args)
    else:
        final_elo = run_evaluation(args)
        print(f"Final Elo: {final_elo}")
